# 队列阻塞问题最终解决方案 (2025-11-09)

## 问题诊断结果

### 版本信息

- **ONNX Runtime**: 1.18.0 ✅ (已降级，不是问题)
- **num_threads**: 4 ❌ (问题根源！)

### 问题现象

从日志可以看到：
- 15:09:15 - 快速发送 10 个数据（几毫秒内）
- 15:10:15 - **1分钟后**，队列阻塞错误

## 根本原因

### ❌ `num_threads: 4` 导致 DataLoader 多进程死锁

**问题**:
1. `num_threads: 4` 导致 DataLoader 使用 4 个 worker 进程
2. 在多进程 CUDA 环境下，这些 worker 进程会竞争 GPU 资源
3. 导致死锁，下游处理停滞
4. 上游继续发送数据，队列被填满
5. 最终触发队列阻塞错误

### ✅ ONNX Runtime 版本不是问题

- ONNX Runtime 1.18.0 已正确安装
- **问题不是 ONNX Runtime 版本，而是 `num_threads` 配置**

## 解决方案

### 必须设置 `num_threads: 0`

**原因**:
1. **避免多进程死锁**: `num_threads: 0` 强制 DataLoader 使用单进程，避免多进程 CUDA 环境下的死锁
2. **队列阻塞问题**: 多进程死锁导致下游处理停滞，队列被填满
3. **性能影响**: 虽然单进程可能稍慢，但在多进程 CUDA 环境下更稳定

### 修复步骤

1. **修改 `opt.txt`**: `num_threads: 4` → `num_threads: 0` ✅
2. **创建自动修复脚本**: `fix_num_threads.sh` ✅
3. **重启服务**: 应用新配置 ✅

## 验证结果

### 配置验证

```bash
# num_threads 配置
num_threads: 0  ✅

# ONNX Runtime 版本
onnxruntime-gpu: 1.18.0  ✅
```

## 为什么不能使用 `num_threads: 4`？

### 技术原因

1. **多进程 CUDA 环境**: 在多进程环境下，多个 worker 进程同时访问 GPU 会导致资源竞争
2. **死锁风险**: DataLoader 的多个 worker 进程可能会相互等待，导致死锁
3. **队列阻塞**: 死锁导致下游处理停滞，上游继续发送数据，队列被填满

### 性能考虑

- **单进程模式** (`num_threads: 0`): 虽然可能稍慢，但更稳定，不会死锁
- **多进程模式** (`num_threads: 4`): 可能更快，但在多进程 CUDA 环境下容易死锁

## 自动修复脚本

### 使用方式

```bash
# 在容器内执行
docker exec starpainting-digital-human-service-1 bash -c "/app/HeyGem-Linux-Python-Hack/fix_num_threads.sh"
```

### 功能

1. 自动检查 `num_threads` 配置
2. 如果不是 0，自动修复为 0
3. 创建备份文件

## 防止再次发生

### 建议

1. **定期检查**: 在服务启动前检查 `num_threads` 配置
2. **自动修复**: 使用 `fix_num_threads.sh` 脚本自动修复
3. **文档说明**: 在配置文件中添加注释说明为什么必须使用 `num_threads: 0`

## 相关文件

- `队列阻塞问题根本原因分析.md` - 详细分析
- `队列阻塞问题紧急修复报告.md` - 修复报告
- `fix_num_threads.sh` - 自动修复脚本
- `版本信息总结.md` - 版本信息
- `opt.txt` - 模型配置文件（已修复）

## 更新日期

2025-11-09

