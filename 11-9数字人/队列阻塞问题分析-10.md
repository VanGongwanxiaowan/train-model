# 队列阻塞问题分析（最新）

## 问题描述
任务 ID:

### 日志分析
```
[2025-11-09 17:29:40,294] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:3]
[2025-11-09 17:29:40,296] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:4]
[2025-11-09 17:29:40,297] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:5]
[2025-11-09 17:29:40,300] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:6]
[2025-11-09 17:29:40,307] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:7]
[2025-11-09 17:29:40,312] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:8]
[2025-11-09 17:29:40,320] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:9]
[2025-11-09 17:29:40,322] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:10]
[2025-11-09 17:30:40,324] [ERROR] [任务视频驱动队列满，严重阻塞，下游队列异常]
```

### 问题现象
1. **数据发送**: 成功发送了10个数据（current_idx:1 到 current_idx:10）
2. **阻塞时间**: 60秒后（17:29:40 到 17:30:40）出现队列阻塞
3. **错误信息**: "任务视频驱动队列满，严重阻塞，下游队列异常"

## 根本原因分析

### 可能原因

#### 1. GPU 未使用或使用不充分
- **现象**: GPU 利用率可能为 0% 或很低
- **原因**: 
  - ONNX Runtime 可能没有真正使用 GPU
  - 服务启动时 LD_LIBRARY_PATH 可能没有正确设置
  - 多进程环境下 GPU 资源可能没有正确分配

#### 2. 处理速度慢
- **现象**: 仅处理了10个数据就阻塞
- **原因**:
  - CPU 处理速度远慢于 GPU
  - 如果使用 CPU，处理150帧视频需要很长时间
  - 队列填满后阻塞

#### 3. 队列配置问题
- **现象**: 队列在60秒后阻塞
- **原因**:
  - 队列大小可能不足
  - 处理速度 < 数据产生速度
  - 下游队列处理慢

#### 4. 多进程同步问题
- **现象**: 队列阻塞
- **原因**:
  - 多进程环境下资源竞争
  - 进程间通信阻塞
  - 同步机制有问题

## 解决方案

### 方案 1: 验证 GPU 使用（最关键）

#### 检查步骤
1. **检查 GPU 利用率**
   ```bash
   nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv
   ```

2. **检查 ONNX Runtime GPU 支持**
   ```bash
   python check_env/check_onnx_cuda.py
   ```

3. **检查服务启动时的环境变量**
   - 确保 LD_LIBRARY_PATH 已设置
   - 确保服务启动时设置了环境变量

### 方案 2: 优化服务启动脚本

#### 问题
- 服务启动时可能没有设置 LD_LIBRARY_PATH
- 环境变量可能没有正确传递

#### 解决方案
创建启动脚本，确保环境变量正确设置：
```bash
#!/bin/bash
source /opt/conda/etc/profile.d/conda.sh
conda activate human
export LD_LIBRARY_PATH=/opt/conda/envs/human/lib:$LD_LIBRARY_PATH
cd /app/HeyGem-Linux-Python-Hack
python app.py
```

### 方案 3: 检查 ONNX Runtime 实际使用

#### 问题
- 虽然检查脚本显示 GPU 可用，但实际运行时可能没有使用
- 需要验证服务运行时的 GPU 使用情况

#### 解决方案
1. **监控 GPU 使用**: 在任务执行时监控 GPU 利用率
2. **检查日志**: 查看是否有 ONNX Runtime GPU 相关错误
3. **验证配置**: 确保 ONNX Runtime 配置正确

### 方案 4: 进一步优化队列配置

#### 问题
- 队列大小可能仍不足
- 处理速度可能仍慢

#### 解决方案
1. **增加队列大小**: 进一步增加队列大小
2. **优化处理逻辑**: 优化视频处理逻辑
3. **增加超时时间**: 进一步增加超时时间

## 诊断步骤

### 步骤 1: 检查 GPU 使用情况
```bash
# 监控 GPU 使用
watch -n 1 'nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv'
```

### 步骤 2: 检查服务环境变量
```bash
# 检查服务进程的环境变量
docker exec starpainting-digital-human-service-1 bash -c "ps e -p \$(ps aux | grep 'python app.py' | grep -v grep | awk '{print \$2}') | grep LD_LIBRARY_PATH"
```

### 步骤 3: 检查 ONNX Runtime 配置
```bash
# 检查 ONNX Runtime 提供者
python check_env/check_onnx_cuda.py
```

### 步骤 4: 查看详细日志
```bash
# 查看任务执行的详细日志
tail -200 /app/HeyGem-Linux-Python-Hack/log/dh.log | grep -E '队列|阻塞|ERROR|GPU'
```

## 预期结果

### 如果 GPU 正常使用
- GPU 利用率 > 0%
- 处理速度应该很快
- 队列不应该阻塞

### 如果 GPU 未使用
- GPU 利用率为 0%
- 处理速度慢
- 队列容易阻塞

## 下一步

### 立即行动
1. **检查 GPU 使用情况**: 监控 GPU 利用率
2. **验证服务环境变量**: 确保 LD_LIBRARY_PATH 已设置
3. **检查 ONNX Runtime**: 验证 GPU 支持
4. **查看详细日志**: 分析任务执行过程

### 后续优化
1. **优化服务启动**: 创建启动脚本确保环境变量正确
2. **监控 GPU 使用**: 持续监控 GPU 使用情况
3. **优化队列配置**: 根据实际情况调整配置
4. **性能测试**: 进行完整的性能测试

