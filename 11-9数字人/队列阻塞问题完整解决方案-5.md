# 队列阻塞问题完整解决方案

## 问题分析

### 问题现象
任务 ID: 996f0706-bd4e-11f0-aa21-0242ac110003

```
[2025-11-09 17:29:40,287] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:1]
[2025-11-09 17:29:40,322] [INFO] [drivered_video >>>>>>>>>>>>>>>>>>>> 发送数据大小:[1], current_idx:10]
[2025-11-09 17:30:40,324] [ERROR] [任务视频驱动队列满，严重阻塞，下游队列异常]
```

### 关键发现
1. **GPU 利用率为 0%**: 所有 GPU 利用率都为 0%
2. **处理速度慢**: 仅处理了 10/150 个数据（6.7%）
3. **队列阻塞**: 60秒后队列填满，阻塞
4. **ONNX Runtime GPU 支持正常**: 检查脚本显示 GPU 可用

## 根本原因

### 核心问题：GPU 未被使用

#### 为什么 GPU 未被使用？

1. **环境变量传递问题**
   - 主进程环境变量已设置
   - 但子进程可能没有正确继承
   - trans_dh_service.so 导入时可能无法找到 cuDNN 库

2. **trans_dh_service.so 是编译后的文件**
   - 文件: `trans_dh_service.cpython-38-x86_64-linux-gnu.so` (16 MB)
   - 在导入时，如果 LD_LIBRARY_PATH 未设置，可能无法找到 cuDNN 库
   - 即使后来设置了环境变量，.so 文件已经加载，无法使用 GPU

3. **多进程环境下环境变量传递**
   - multiprocessing.spawn 方式创建的子进程会继承环境变量
   - 但如果 .so 文件在导入时已经加载，环境变量可能无效

4. **ONNX Runtime 实际运行时回退到 CPU**
   - 虽然检查脚本显示 GPU 可用，但实际运行时可能因为环境变量问题回退到 CPU
   - 处理速度慢，导致队列阻塞

### 为什么会导致队列阻塞？

1. **处理速度慢**
   - GPU 处理: 每帧几毫秒
   - CPU 处理: 每帧几秒
   - 速度差距: GPU 比 CPU 快 10-100 倍

2. **队列填满**
   - 数据产生速度快: 150 帧数据快速进入队列
   - 处理速度慢: CPU 处理速度远慢于数据产生速度
   - 队列填满: 仅处理了 10/150 个数据就阻塞

3. **阻塞机制**
   - 队列填满后阻塞
   - 无法继续处理
   - 60秒后报错

## 解决方案

### 方案 1: 确保服务启动时环境变量正确设置（最关键）

#### 问题
- 服务启动时 LD_LIBRARY_PATH 必须正确设置
- trans_dh_service.so 导入时需要能找到 cuDNN 库

#### 解决方案
使用启动脚本 `启动服务.sh`，确保环境变量在导入 .so 文件之前设置：
```bash
#!/bin/bash
source /opt/conda/etc/profile.d/conda.sh
conda activate human
export LD_LIBRARY_PATH=/opt/conda/envs/human/lib:$LD_LIBRARY_PATH
export ORT_PARALLEL=0
export OMP_WAIT_POLICY=ACTIVE
export ORT_LOGGING_LEVEL=3
cd /app/HeyGem-Linux-Python-Hack
python app.py
```

#### 为什么有效？
- **确保环境变量设置**: 启动脚本确保所有环境变量在导入 .so 文件之前设置
- **传递给子进程**: 环境变量会传递给所有子进程
- **ONNX Runtime 能找到库**: LD_LIBRARY_PATH 确保 cuDNN 库能被找到

### 方案 2: 在 app.py 中提前设置环境变量

#### 问题
- app.py 中环境变量设置可能在导入 trans_dh_service 之后
- 需要确保在导入之前设置

#### 解决方案
在 app.py 开头，在所有导入之前设置环境变量：
```python
# 在所有导入之前设置环境变量
import os
conda_lib_path = "/opt/conda/envs/human/lib"
current_ld_path = os.environ.get("LD_LIBRARY_PATH", "")
if conda_lib_path not in current_ld_path:
    if current_ld_path:
        os.environ["LD_LIBRARY_PATH"] = f"{conda_lib_path}:{current_ld_path}"
    else:
        os.environ["LD_LIBRARY_PATH"] = conda_lib_path
    os.putenv("LD_LIBRARY_PATH", os.environ["LD_LIBRARY_PATH"])
```

#### 为什么有效？
- **提前设置**: 在所有导入之前设置环境变量
- **确保 .so 文件能找到库**: trans_dh_service.so 导入时能找到 cuDNN 库
- **传递给子进程**: 环境变量会传递给所有子进程

### 方案 3: 检查 trans_dh_service.so 的 GPU 支持

#### 问题
- trans_dh_service.so 是编译后的文件
- 可能编译时未链接 GPU 库
- 即使环境变量正确，也可能无法使用 GPU

#### 解决方案
1. **检查 .so 文件依赖**: 使用 `ldd` 检查依赖
2. **检查编译配置**: 查看编译时的配置
3. **重新编译**: 如果必要，重新编译以支持 GPU

### 方案 4: 监控实际运行时的 GPU 使用

#### 问题
- 检查脚本显示 GPU 可用，但实际运行时可能没有使用
- 需要验证实际运行时的 GPU 使用情况

#### 解决方案
1. **监控 GPU 使用**: 在任务执行时监控 GPU 利用率
2. **检查日志**: 查看是否有 ONNX Runtime GPU 相关错误
3. **验证配置**: 确保 ONNX Runtime 配置正确

## 实施步骤

### 步骤 1: 使用启动脚本重启服务 ✅

```bash
# 停止服务
ps aux | grep 'python app.py' | grep -v grep | awk '{print $2}' | xargs -r kill -9

# 使用启动脚本重启服务
bash /app/HeyGem-Linux-Python-Hack/启动服务.sh
```

### 步骤 2: 验证环境变量 ✅

```bash
# 检查服务进程的环境变量
ps e -p $(ps aux | grep 'python app.py' | grep -v grep | awk '{print $2}') | grep LD_LIBRARY_PATH
```

### 步骤 3: 监控 GPU 使用 ⏳

```bash
# 监控 GPU 使用
watch -n 1 'nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv'
```

### 步骤 4: 测试任务执行 ⏳

- 提交测试任务
- 观察 GPU 使用情况
- 检查队列阻塞情况

## 预期结果

### 如果 GPU 正常使用
- GPU 利用率 > 0%
- 处理速度应该很快（每帧几毫秒）
- 队列不应该阻塞
- 150 帧视频应该在几分钟内完成

### 如果 GPU 未使用
- GPU 利用率为 0%
- 处理速度慢（每帧几秒）
- 队列容易阻塞
- 150 帧视频可能需要几十分钟

## 总结

### 根本原因
**GPU 未被使用** - 这是队列阻塞的根本原因
- GPU 利用率为 0%
- 任务在 CPU 上运行
- 处理速度慢，导致队列阻塞

### 关键问题
1. **环境变量传递**: 子进程可能没有正确继承环境变量
2. **.so 文件加载**: trans_dh_service.so 导入时可能无法找到 cuDNN 库
3. **多进程环境**: multiprocessing.spawn 方式可能有问题

### 解决方案
1. **使用启动脚本**: 确保环境变量在导入 .so 文件之前设置
2. **提前设置环境变量**: 在 app.py 开头设置环境变量
3. **监控 GPU 使用**: 验证 GPU 是否被使用
4. **优化配置**: 根据实际情况调整配置
