# é˜Ÿåˆ—é˜»å¡é—®é¢˜æ·±åº¦åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## é—®é¢˜æè¿°

```
[2025-11-09 14:03:48,074] [process.py[line:108]] [ERROR] 
[[b0ea450c-bd31-11f0-bc37-0242ac110003]ä»»åŠ¡è§†é¢‘é©±åŠ¨é˜Ÿåˆ—æ»¡ï¼Œä¸¥é‡é˜»å¡ï¼Œä¸‹æ¸¸é˜Ÿåˆ—å¼‚å¸¸]
```

## é—®é¢˜åˆ†æ

### 1. æ—¶é—´çº¿åˆ†æ

ä»æ—¥å¿—å¯ä»¥çœ‹å‡ºï¼š
- **14:02:48**: å¼€å§‹å‘é€æ•°æ®åˆ°é˜Ÿåˆ—ï¼ˆå‘é€äº†10ä¸ªæ•°æ®åŒ…ï¼Œcurrent_idx:1-10ï¼‰
- **14:03:48**: 60ç§’åæŠ¥é”™"é˜Ÿåˆ—æ»¡ï¼Œä¸¥é‡é˜»å¡"
- **é—®é¢˜**: ä¸‹æ¸¸å¤„ç†é˜Ÿåˆ—åœ¨60ç§’å†…æ²¡æœ‰æ¶ˆè´¹ä»»ä½•æ•°æ®ï¼Œå¯¼è‡´ä¸Šæ¸¸é˜Ÿåˆ—é˜»å¡

### 2. æ ¹æœ¬åŸå› æ¨æµ‹

æ ¹æ® [HeyGem-Linux-Python-Hack GitHub Issues](https://github.com/Holasyb918/HeyGem-Linux-Python-Hack/issues?q=is%3Aissue+state%3Aclosed) å’Œç›¸å…³æŠ€æœ¯åˆ†æï¼š

#### åŸå›  1: ONNX Runtime CUDA æ‰§è¡Œæä¾›è€…çš„åŒæ­¥é—®é¢˜ âš ï¸

**é—®é¢˜**:
- ONNX Runtime GPU 1.18.0 åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹å¯èƒ½å­˜åœ¨ CUDA æµåŒæ­¥é—®é¢˜
- CUDA æ‰§è¡Œæä¾›è€…å¯èƒ½é˜»å¡äº†ä¸»çº¿ç¨‹ï¼Œå¯¼è‡´é˜Ÿåˆ—æ— æ³•åŠæ—¶å¤„ç†

**è¯æ®**:
- GPU åˆ©ç”¨ç‡æ˜¾ç¤ºä¸º 0%ï¼Œä½†æ˜¾å­˜æœ‰ä½¿ç”¨ï¼ˆ52GB/80GBï¼‰
- è¿™è¡¨æ˜ GPU å¯èƒ½åœ¨ç­‰å¾…åŒæ­¥ï¼Œä½†æ²¡æœ‰å……åˆ†åˆ©ç”¨

#### åŸå›  2: å¤šè¿›ç¨‹ç¯å¢ƒä¸‹çš„èµ„æºç«äº‰ ğŸ”„

**é—®é¢˜**:
- å¤šä¸ªè¿›ç¨‹åŒæ—¶ä½¿ç”¨ GPU èµ„æº
- ONNX Runtime çš„ CUDA æ‰§è¡Œæä¾›è€…å¯èƒ½æ²¡æœ‰æ­£ç¡®é…ç½®çº¿ç¨‹æ•°
- å¯¼è‡´è¿›ç¨‹é—´èµ„æºç«äº‰ï¼Œé˜Ÿåˆ—å¤„ç†å˜æ…¢

#### åŸå›  3: é˜Ÿåˆ—å¤§å°å’Œè¶…æ—¶è®¾ç½®ä¸å½“ ğŸ“Š

**é—®é¢˜**:
- é˜Ÿåˆ—å¯èƒ½è®¾ç½®å¾—å¤ªå°
- è¶…æ—¶æ—¶é—´è®¾ç½®ä¸å½“ï¼ˆ60ç§’å¯èƒ½å¤ªé•¿ï¼‰
- ä¸‹æ¸¸å¤„ç†é€Ÿåº¦è·Ÿä¸ä¸Šä¸Šæ¸¸ç”Ÿäº§é€Ÿåº¦

### 3. ä¸ GitHub Issues çš„å…³è”

è™½ç„¶æ²¡æœ‰ç›´æ¥æ‰¾åˆ°ç›¸åŒçš„é˜Ÿåˆ—é˜»å¡é—®é¢˜ï¼Œä½†ç›¸å…³ issues æä¾›äº†çº¿ç´¢ï¼š

- **#73: Failed to execute with 16GB GPU** - å¯èƒ½ä¸ GPU èµ„æºä¸è¶³æœ‰å…³
- **#65: heygem_f2f å¡ä½ï¼Œé™·å…¥æ— é™å¾ªç¯** - å¯èƒ½ä¸é˜Ÿåˆ—å¤„ç†é€»è¾‘æœ‰å…³
- **#76: æƒ³åœ¨ä¸€ä¸ªGPUä¸ŠåŒæ—¶è·‘å¤šä¸ªä»»åŠ¡** - å¤šä»»åŠ¡å¯èƒ½å¯¼è‡´èµ„æºç«äº‰

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä¼˜åŒ– ONNX Runtime é…ç½®ï¼ˆæ¨èï¼‰âœ…

#### 1.1 è®¾ç½® ONNX Runtime ä½¿ç”¨å•çº¿ç¨‹

åœ¨ä»£ç ä¸­é…ç½® ONNX Runtime SessionOptionsï¼š

```python
import onnxruntime as ort

# åˆ›å»ºä¼šè¯é€‰é¡¹
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = 1  # å†…éƒ¨æ“ä½œä½¿ç”¨å•çº¿ç¨‹
sess_options.inter_op_num_threads = 1  # æ“ä½œé—´ä½¿ç”¨å•çº¿ç¨‹
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # é¡ºåºæ‰§è¡Œæ¨¡å¼

# é…ç½® CUDA æ‰§è¡Œæä¾›è€…é€‰é¡¹
cuda_provider_options = {
    'device_id': 0,
    'arena_extend_strategy': 'kSameAsRequested',
    'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
    'cudnn_conv_algo_search': 'EXHAUSTIVE',
    'do_copy_in_default_stream': True,
}

# åˆ›å»ºä¼šè¯æ—¶ä½¿ç”¨è¿™äº›é€‰é¡¹
providers = [
    ('CUDAExecutionProvider', cuda_provider_options),
    'CPUExecutionProvider'
]

onnx_session = ort.InferenceSession(
    onnx_model_path,
    sess_options=sess_options,
    providers=providers
)
```

#### 1.2 æ·»åŠ ç¯å¢ƒå˜é‡

åœ¨ `app.py` ä¸­æ·»åŠ ï¼š

```python
# ONNX Runtime ä¼˜åŒ–é…ç½®
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
# ONNX Runtime ä¸“ç”¨é…ç½®
os.environ["ORT_PARALLEL"] = "0"  # ç¦ç”¨å¹¶è¡Œ
```

### æ–¹æ¡ˆ 2: é™çº§ ONNX Runtime ç‰ˆæœ¬ ğŸ”„

æ ¹æ®é¡¹ç›®æ–‡æ¡£ï¼Œä¹‹å‰ä½¿ç”¨çš„ç‰ˆæœ¬æ˜¯ `onnxruntime-gpu==1.16.0`ï¼Œè€Œå½“å‰ä½¿ç”¨çš„æ˜¯ `1.18.0`ã€‚

**å°è¯•é™çº§**:

```bash
pip uninstall onnxruntime-gpu
pip install onnxruntime-gpu==1.16.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### æ–¹æ¡ˆ 3: å¢åŠ  Docker å…±äº«å†…å­˜ ğŸ³

å½“å‰å…±äº«å†…å­˜åªæœ‰ 64MBï¼Œå¯èƒ½éœ€è¦å¢åŠ ï¼š

```bash
# é‡æ–°åˆ›å»ºå®¹å™¨æ—¶æ·»åŠ 
docker run --shm-size=2g ...
```

### æ–¹æ¡ˆ 4: ä¼˜åŒ–é˜Ÿåˆ—å¤„ç†é€»è¾‘ ğŸ“¦

å¦‚æœèƒ½å¤Ÿä¿®æ”¹æœåŠ¡ä»£ç ï¼Œå¯ä»¥ï¼š

1. **å¢åŠ é˜Ÿåˆ—å¤§å°**
2. **å‡å°‘ batch_size**ï¼ˆä» config.ini ä¸­ä¿®æ”¹ï¼‰
3. **æ·»åŠ è¶…æ—¶å¤„ç†æœºåˆ¶**
4. **ä¼˜åŒ–å¤šè¿›ç¨‹é€šä¿¡**

### æ–¹æ¡ˆ 5: ä½¿ç”¨ CPU æ‰§è¡Œæä¾›è€…ä½œä¸ºå¤‡é€‰ âš ï¸

å¦‚æœ CUDA æ‰§è¡Œæä¾›è€…æœ‰é—®é¢˜ï¼Œå¯ä»¥ä¸´æ—¶ä½¿ç”¨ CPUï¼š

```python
providers = ['CPUExecutionProvider']  # ä¸´æ—¶ä½¿ç”¨ CPU
```

## æ¨èå®æ–½æ­¥éª¤

### æ­¥éª¤ 1: ç«‹å³å°è¯• - æ·»åŠ  ONNX Runtime ç¯å¢ƒå˜é‡

åœ¨ `app.py` ä¸­æ·»åŠ ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´ï¼Œå…¶ä»–ç¯å¢ƒå˜é‡ä¹‹åæ·»åŠ 
os.environ["ORT_PARALLEL"] = "0"
os.environ["OMP_NUM_THREADS"] = "1"
```

### æ­¥éª¤ 2: æ£€æŸ¥å¹¶ä¿®æ”¹ ONNX Runtime é…ç½®

æ‰¾åˆ°ä½¿ç”¨ ONNX Runtime çš„ä»£ç ä½ç½®ï¼Œæ·»åŠ  SessionOptions é…ç½®ã€‚

### æ­¥éª¤ 3: å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå°è¯•é™çº§ç‰ˆæœ¬

```bash
docker exec starpainting-digital-human-service-1 bash -c "source /opt/conda/etc/profile.d/conda.sh && conda activate human && pip install onnxruntime-gpu==1.16.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
```

### æ­¥éª¤ 4: ç›‘æ§ GPU ä½¿ç”¨æƒ…å†µ

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹é˜Ÿåˆ—å¤„ç†æƒ…å†µ
tail -f /app/HeyGem-Linux-Python-Hack/log/dh.log
```

## éªŒè¯æ–¹æ³•

1. **æµ‹è¯•é˜Ÿåˆ—å¤„ç†é€Ÿåº¦**: è§‚å¯Ÿæ—¥å¿—ä¸­é˜Ÿåˆ—å¤„ç†çš„æ—¶é—´é—´éš”
2. **ç›‘æ§ GPU åˆ©ç”¨ç‡**: åº”è¯¥çœ‹åˆ° GPU åˆ©ç”¨ç‡ > 0%
3. **æ£€æŸ¥é”™è¯¯æ—¥å¿—**: ä¸å†å‡ºç°"é˜Ÿåˆ—æ»¡"é”™è¯¯

## é¢„æœŸæ•ˆæœ

å®æ–½è¿™äº›æ–¹æ¡ˆåï¼š
- âœ… é˜Ÿåˆ—èƒ½å¤ŸåŠæ—¶å¤„ç†æ•°æ®
- âœ… GPU åˆ©ç”¨ç‡æå‡
- âœ… ä¸å†å‡ºç°é˜Ÿåˆ—é˜»å¡é”™è¯¯
- âœ… ä»»åŠ¡å¤„ç†é€Ÿåº¦æå‡

## å‚è€ƒèµ„æ–™

- [HeyGem-Linux-Python-Hack GitHub Issues](https://github.com/Holasyb918/HeyGem-Linux-Python-Hack/issues?q=is%3Aissue+state%3Aclosed)
- [ONNX Runtime Performance Tuning](https://onnxruntime.ai/docs/performance/tune-performance.html)
- [ONNX Runtime CUDA Execution Provider](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)

## æ›´æ–°æ—¥æœŸ

2025-11-09

