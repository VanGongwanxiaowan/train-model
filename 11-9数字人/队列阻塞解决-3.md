# 队列阻塞问题根本解决方案

## 问题分析

### 现象

从日志 `dh.log` (167-179行) 可以看到：

```
[14:15:56] 任务视频驱动队列启动 batch_size:1, len:150
[14:15:56] 开始循环，发送数据大小:[1], current_idx:1-10
[14:16:56] 60秒后报错: 任务视频驱动队列满，严重阻塞，下游队列异常
```

### 关键问题

1. **时间线分析**：
   - 14:15:56: 开始发送数据到队列（10个数据包在不到1秒内发送完成）
   - 14:16:56: 60秒后报错"队列满，严重阻塞"
   - **问题**: 下游队列在60秒内完全没有消费任何数据

2. **系统状态**：
   - GPU 利用率: 0%（GPU 没有在工作）
   - GPU 显存: 52GB/80GB（显存被占用，但GPU空闲）
   - 共享内存: 64MB（太小）
   - 多进程启动方式: `fork`（可能导致死锁）
   - DataLoader num_workers: 4（可能导致多进程死锁）

3. **根本原因**：

#### 原因 1: PyTorch DataLoader 多进程死锁 🔴

**问题**: 
- DataLoader 使用了 `num_workers=4`（从 `opt.txt` 看到 `num_threads: 4`）
- 在多进程环境下，DataLoader 的多工作进程可能导致死锁
- 特别是在使用 CUDA 时，fork 启动方式 + DataLoader 多进程 = 死锁

**证据**:
- 有多个僵尸进程（defunct processes）
- GPU 利用率 0%，但显存被占用
- 队列完全阻塞，下游没有消费

#### 原因 2: 多进程启动方式问题 ⚠️

**问题**:
- 当前使用 `fork` 启动方式
- `fork` 在多进程 + CUDA 环境下有已知问题
- 子进程继承父进程的 CUDA 上下文，可能导致死锁

#### 原因 3: 共享内存不足 📊

**问题**:
- 当前共享内存只有 64MB
- PyTorch DataLoader 多进程需要共享内存来传递数据
- 共享内存不足可能导致进程阻塞

#### 原因 4: CUDA 流同步问题 🔄

**问题**:
- GPU 利用率 0%，但显存被占用
- 说明 GPU 可能在等待 CUDA 流同步
- 多进程环境下的 CUDA 同步可能导致死锁

## 解决方案

### 方案 1: 强制 DataLoader 使用单进程模式（最有效）✅

**实施**: 在环境变量中强制设置 DataLoader 使用单进程

```python
# 在 app.py 中添加
os.environ["TORCH_DATALOADER_NUM_WORKERS"] = "0"  # 强制 DataLoader 使用单进程
```

**原因**: 
- DataLoader 的 `num_workers > 0` 会在多进程环境下创建子进程
- 这些子进程可能导致死锁，特别是与 CUDA 结合时
- 设置为 0 可以避免多进程问题，虽然可能稍微慢一点，但更稳定

### 方案 2: 修改模型配置中的 num_threads

**问题**: `landmark2face_wy/checkpoints/test/opt.txt` 中 `num_threads: 4`

**解决方案**: 需要修改代码，在加载配置时强制设置 `num_threads=0`

### 方案 3: 增加 Docker 共享内存 🐳

**当前**: 64MB
**建议**: 至少 2GB

```bash
# 重新创建容器时添加
docker run --shm-size=2g ...
```

### 方案 4: 优化环境变量配置 ✅

已添加的环境变量：
- `TORCH_DATALOADER_NUM_WORKERS=0` - 强制单进程
- `OMP_NUM_THREADS=1` - 限制 OpenMP 线程
- `TORCH_NUM_THREADS=1` - 限制 PyTorch 线程
- `ORT_PARALLEL=0` - 禁用 ONNX Runtime 并行

### 方案 5: 清理僵尸进程

```bash
# 清理僵尸进程
docker exec starpainting-digital-human-service-1 bash -c "ps aux | grep defunct | awk '{print \$2}' | xargs -r kill -9 2>/dev/null || true"
```

## 实施步骤

### 步骤 1: 添加环境变量（已实施）✅

已在 `app.py` 中添加 `TORCH_DATALOADER_NUM_WORKERS=0`

### 步骤 2: 检查并修改模型配置

需要检查代码中加载 `opt.txt` 的地方，确保 `num_threads` 被设置为 0。

### 步骤 3: 增加共享内存

如果可能，重新创建容器并增加共享内存。

### 步骤 4: 重启服务

```bash
docker exec starpainting-digital-human-service-1 pkill -f "python app.py"
sleep 2
docker exec starpainting-digital-human-service-1 bash -c "source /opt/conda/etc/profile.d/conda.sh && conda activate human && cd /app/HeyGem-Linux-Python-Hack && nohup python app.py > /tmp/gradio.log 2>&1 &"
```

## 验证方法

1. **检查环境变量**:
```bash
docker exec starpainting-digital-human-service-1 bash -c "source /opt/conda/etc/profile.d/conda.sh && conda activate human && python -c 'import os; print(\"TORCH_DATALOADER_NUM_WORKERS:\", os.environ.get(\"TORCH_DATALOADER_NUM_WORKERS\", \"未设置\"))'"
```

2. **测试任务处理**:
   - 上传音频和视频文件
   - 观察是否还有队列阻塞错误
   - 观察 GPU 利用率是否提升

3. **监控日志**:
```bash
docker exec starpainting-digital-human-service-1 tail -f /app/HeyGem-Linux-Python-Hack/log/dh.log
```

## 预期效果

实施这些方案后：
- ✅ DataLoader 使用单进程模式，避免多进程死锁
- ✅ 队列能够及时处理数据
- ✅ GPU 利用率提升
- ✅ 不再出现队列阻塞错误
- ✅ 不再有僵尸进程

## 技术依据

1. **PyTorch 官方文档**: DataLoader 的 `num_workers > 0` 在多进程环境下可能导致死锁
2. **GitHub Issues**: 多个 issues 提到多进程 + CUDA 的死锁问题
3. **实践经验**: 在 Docker 容器中，使用 `num_workers=0` 更稳定

## 更新日期

2025-11-09

