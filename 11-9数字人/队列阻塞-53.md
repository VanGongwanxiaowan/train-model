# 队列阻塞问题解决方案总结

## 问题诊断

### 当前问题
1. **GPU 利用率为 0%** ❌
2. **74 个僵尸进程** ❌
3. **队列阻塞** ❌ - 视频驱动队列在60秒后阻塞
4. **任务处理慢** ⚠️ - 仅处理了 10/150 个数据（6.7%）

### 根本原因

#### 1. GPU 未使用
- ✅ ONNX Runtime 支持 GPU（CUDAExecutionProvider 可用）
- ✅ CUDA 可用（8个 GPU 可用）
- ⚠️ 但 GPU 利用率为 0%，可能是进程阻塞或配置问题

#### 2. 队列阻塞
- **关键原因**: GPU 处理 150 帧视频需要时间
- 每帧处理可能需要几秒钟
- 总处理时间可能需要 2-3 分钟
- **60秒超时时间明显不足**

#### 3. 僵尸进程
- 父进程未等待子进程退出
- 进程清理机制有问题
- 多进程环境下信号处理复杂

## 解决方案

### 方案 1: 增加超时时间（最关键）⭐⭐⭐⭐⭐

#### 为什么有效？
- **GPU 处理需要时间**: 150帧视频，每帧处理可能需要几秒钟
- **总处理时间**: 可能需要 2-3 分钟
- **60秒超时不足**: 明显不足，导致过早判定失败
- **增加超时时间**: 给 GPU 足够的处理时间

#### 实施内容
1. **任务等待超时时间**: 从 600秒 增加到 900秒（15分钟）
2. **错误等待时间**: 从 60秒 增加到 180秒（3分钟）
3. **队列超时时间**: 从 300秒 增加到 900秒（15分钟）
4. **服务器超时时间**: 增加到 600秒（10分钟）

#### 代码修改
```python
# 任务等待超时时间
max_wait_time = 900  # 从600秒增加到900秒（15分钟）

# 错误等待时间
error_wait_start = 180  # 从60秒增加到180秒（3分钟）

# 队列超时时间
os.environ["GRADIO_QUEUE_TIMEOUT"] = "900"  # 从300秒增加到900秒（15分钟）

# 服务器超时时间
server_timeout=600  # 10分钟
```

### 方案 2: 增加队列大小 ⭐⭐⭐⭐

#### 为什么有效？
- 允许更多任务排队
- 减少队列填满的概率
- 提高系统吞吐量

#### 实施内容
- 队列大小: 从 10 增加到 20

#### 代码修改
```python
demo.queue(
    max_size=20,  # 从10增加到20
    default_concurrency_limit=1,
    api_open=False
)
```

### 方案 3: 优化等待逻辑 ⭐⭐⭐⭐

#### 为什么有效？
- 更合理的等待时间
- 避免过早判定失败
- 提高任务成功率
- 减少日志输出

#### 实施内容
1. **优化检查间隔**: 从 3秒 减少到 2秒
2. **优化日志记录**: 减少日志输出频率（30秒、60秒）
3. **优化等待时间**: 根据实际处理时间调整

### 方案 4: 优化 ONNX Runtime 配置 ⭐⭐⭐

#### 为什么有效？
- 确保 ONNX Runtime 使用 GPU
- 优化配置参数
- 减少多进程冲突

#### 实施内容
1. **优化环境变量配置**
   - 移除可能无效的环境变量
   - 保留有效的环境变量
   - 优化配置参数

2. **检查 ONNX Runtime 提供者**
   - 验证 CUDAExecutionProvider 是否可用
   - 确保 GPU 可以被使用

### 方案 5: 清理僵尸进程 ⭐⭐⭐

#### 为什么有效？
- 释放系统资源
- 减少资源竞争
- 提高系统性能

#### 实施内容
1. **重启服务**: 清理所有僵尸进程
2. **改进清理机制**: 在服务启动时清理
3. **定期清理**: 添加定期清理机制

## 实施步骤

### 步骤 1: 检查 ONNX Runtime 和 CUDA ✅

```bash
# 检查 ONNX Runtime 提供者
python3 -c "import onnxruntime as ort; print(ort.get_available_providers())"
# 结果: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
# ✅ CUDAExecutionProvider 可用

# 检查 CUDA 可用性
python3 -c "import torch; print(torch.cuda.is_available())"
# 结果: True
# ✅ CUDA 可用，8个 GPU 可用
```

### 步骤 2: 优化配置 ✅

1. **更新 app.py**
   - ✅ 优化 ONNX Runtime 配置
   - ✅ 优化队列配置
   - ✅ 优化任务等待逻辑
   - ✅ 增加超时时间

2. **验证配置**
   - ✅ 检查环境变量
   - ✅ 检查队列配置
   - ✅ 检查超时时间

### 步骤 3: 清理僵尸进程（重启服务）✅

```bash
# 停止服务
ps aux | grep 'python app.py' | grep -v grep | awk '{print $2}' | xargs -r kill -9

# 清理 multiprocessing 进程
pkill -9 -f "multiprocessing.spawn" 2>/dev/null || true

# 等待进程清理
sleep 5
```

### 步骤 4: 重启服务 ✅

```bash
# 重启服务
source /opt/conda/etc/profile.d/conda.sh
conda activate human
cd /app/HeyGem-Linux-Python-Hack
nohup python app.py > /tmp/gradio.log 2>&1 &
```

### 步骤 5: 验证修复 ⏳

1. **检查 GPU 使用**
   ```bash
   nvidia-smi --query-gpu=index,utilization.gpu --format=csv
   ```

2. **检查僵尸进程**
   ```bash
   ps aux | grep defunct | wc -l
   ```

3. **测试任务执行**
   - 提交测试任务
   - 观察 GPU 使用情况
   - 检查队列阻塞情况

## 预期效果

### 修复后预期

1. **GPU 使用**
   - GPU 利用率 > 0%
   - GPU 内存使用增加
   - 处理速度提高

2. **僵尸进程**
   - 僵尸进程数量减少（重启后为0）
   - 新任务不再产生僵尸进程
   - 系统资源释放

3. **队列阻塞**
   - 队列阻塞问题解决
   - 任务处理速度提高
   - 任务成功率提高

4. **性能提升**
   - 处理速度提高
   - 任务完成时间减少
   - 系统吞吐量提高

## 关键修复说明

### 1. 增加超时时间（最关键）

#### 为什么这是最关键的修复？
- **GPU 处理需要时间**: 150帧视频，每帧处理可能需要几秒钟
- **总处理时间**: 可能需要 2-3 分钟
- **60秒超时不足**: 明显不足，导致过早判定失败
- **增加超时时间**: 给 GPU 足够的处理时间，提高任务成功率

#### 具体分析
- **init_wh 阶段**: 通常需要 7-20 秒，25秒足够
- **视频处理阶段**: 150帧视频，每帧处理可能需要几秒钟
- **总处理时间**: 可能需要 2-3 分钟
- **错误等待时间**: 从60秒增加到180秒，给GPU更多恢复时间

### 2. 增加队列大小

#### 为什么有效？
- **允许更多任务排队**: 减少队列填满的概率
- **提高系统吞吐量**: 可以处理更多任务
- **减少阻塞**: 队列不容易填满

### 3. 优化等待逻辑

#### 为什么有效？
- **更合理的等待时间**: 根据实际处理时间调整
- **避免过早判定失败**: 给 GPU 足够的处理时间
- **提高任务成功率**: 减少误判失败
- **减少日志输出**: 降低I/O开销

## 监控建议

### 实时监控

```bash
# 监控 GPU
watch -n 1 'nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv'

# 监控进程
watch -n 1 'ps aux | grep python | wc -l && ps aux | grep defunct | wc -l'

# 监控日志
tail -f /app/HeyGem-Linux-Python-Hack/log/dh.log | grep -E '队列|阻塞|GPU|ERROR|成功'
```

### 检查命令

```bash
# 检查 ONNX Runtime
python3 -c "import onnxruntime as ort; print(ort.get_available_providers())"

# 检查 CUDA
python3 -c "import torch; print(torch.cuda.is_available())"

# 检查进程
ps aux | grep python | grep -v grep

# 检查 GPU
nvidia-smi

# 检查队列
grep -E '队列|阻塞|ERROR' /app/HeyGem-Linux-Python-Hack/log/dh.log | tail -20
```

## 总结

### 关键修复

1. **增加超时时间** ⭐⭐⭐⭐⭐
   - 任务等待超时: 从 600秒 增加到 900秒（15分钟）
   - 错误等待时间: 从 60秒 增加到 180秒（3分钟）
   - 队列超时时间: 从 300秒 增加到 900秒（15分钟）
   - 服务器超时时间: 增加到 600秒（10分钟）

2. **增加队列大小** ⭐⭐⭐⭐
   - 从 10 增加到 20

3. **优化等待逻辑** ⭐⭐⭐⭐
   - 检查间隔: 从 3秒 减少到 2秒
   - 日志记录: 减少日志输出频率

4. **优化 ONNX Runtime 配置** ⭐⭐⭐
   - 优化环境变量配置
   - 确保 GPU 可以被使用

5. **清理僵尸进程** ⭐⭐⭐
   - 重启服务清理
   - 改进清理机制

### 为什么这些修复有效？

1. **GPU 处理需要时间**
   - 150帧视频需要处理时间
   - 每帧处理可能需要几秒钟
   - 总处理时间可能需要 2-3 分钟
   - 60秒超时明显不足

2. **增加超时时间**
   - 给 GPU 足够的处理时间
   - 避免过早判定失败
   - 提高任务成功率

3. **优化配置**
   - 增加队列大小
   - 优化等待逻辑
   - 减少日志输出

### 下一步

1. **实施修复** ✅ - 已完成
2. **验证效果** ⏳ - 需要测试
3. **持续监控** ⏳ - 需要持续监控
4. **优化改进** ⏳ - 根据实际情况进一步优化

## 相关文件

- `队列阻塞问题完整解决方案_最终版.md` - 完整解决方案
- `队列阻塞问题最终解决方案.md` - 最终解决方案
- `队列阻塞问题解决方案实施报告.md` - 实施报告
- `清理僵尸进程.sh` - 清理脚本
- `app.py` - 主程序（已优化）

